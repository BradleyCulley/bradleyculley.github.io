I think AI will automate all, or almost all jobs at about the same time. In other words, a step-change profile for the technological singularity.

Reasoning:
<ul>
    <li>
        I think the different jobs people do are fundamentally not that computationally distinct. Being a lawyer versus being a construction worker versus being an engineer.
        Further evidence, in both AIs and humans, is what in AI is called “transfer learning”. It's easier for an engineer to become a doctor, than it is for a random person to do so. The same for engineer to lawyer, even.
        <br/><br/>
    </li>
    <li>
        The jobs humans do have already been optimized around the parts of the computational universe (the ruliad) that are currently unautomatable.
        <br/><br/>
        Joseph Schumpeter's "Creative destruction" and David Frederick Schloss' "Lump of Labor Fallacy.
        <br/><br/>
        As we've automated more, to date there's paradoxically been more to do. Joseph Schumpeter's "Creative destruction" is about
        <br/><br/>
        "the process that sees new innovations replacing existing ones that are rendered obsolete over time." -https://www.cmu.edu/epp/irle/irle-blog-pages/schumpeters-theory-of-creative-destruction.html
        <br/><br/>
        "As an example, in the late 1800s and early 1900s incremental improvements to horse and buggy transportation continued to be valuable, and innovations in the buggy and buggy whip could fetch a considerable price in the market.  With the introduction of Ford’s Model T in 1908, however, these “technologies” were effectively driven out by a superior innovation.  Over time, newer and better innovations will continue to drive out worse ones, just as the Model T did the horse and buggy and numerous iterations of vehicles have subsequently driven out the Model T and generations of its successors." -https://www.cmu.edu/epp/irle/irle-blog-pages/schumpeters-theory-of-creative-destruction.html
        <br/><br/>
        The point of "Creative Destruction" is that the net effect is not the net destruction of economic value, nor of jobs.
        New economic niches, value propositions, and computational complexity spring from the wave of new technology.
        <br/><br/>
        Closely related is David Frederick Schloss' "Lump of Labor Fallacy".
        <br/><br/>
        According to The St. Louis Federal Reserve:
        "The lump of labor fallacy is the assumption that there is a fixed amount of work to be done. If this were true, new jobs could not be generated, just redistributed. Those who believe the fallacy have often felt threatened by new technology or the entrance of new people into the labor force. These fears are rooted in a mistaken zero-sum view of the economy, which holds that when someone gains in a transaction, someone else loses. It's a tempting idea to some because it seems to be true. For example, jobs can be lost to automation and immigration. However, that is not the full story. In reality, the demand for labor is not fixed. Changes in one industry can be offset, or overshadowed, by growth in another. And as the labor force grows, total employment increases too (Figure 1)."
        -https://research.stlouisfed.org/publications/page1-econ/2020/11/02/examining-the-lump-of-labor-fallacy-using-a-simple-economic-model
        <br/><br/>
        <div align="center">
            <img src="https://bradleyculley.github.io/images/labor_force_and_employment.png" />
        </div>
        <br/><br/>
        In fact, there are more job openings than unemployed people, part of a 10-year trend.
        The red line going below "1" in the graph below means there are more jobs than unemployed people.
        It's risen slightly recently, but was at an incredible 0.5 for a few years, starting in 2021.
        <br/><br/>
        <div align="center">
            <img src="https://bradleyculley.github.io/images/unemployed_people_per_job_opening.png" />
        </div>
        <br/><br/>
        Creative Destruction and Lump of Labor aren't laws of nature though. They could break down any time.<br/><br/>
        Connectionism, however, is actual science. Connectionism is the idea that thinking, memory, etc. are all enabled by the patterns of connections between neurons in the brain.
        This has been known for quite some time. How does connectionism factor into this?
        It means there's a (general) mechanistic understanding of cognition, as well as quantifiable upper limit to the cognition and intelligence of a given human.
        <br/><br/>
        Moore's Law may as well be a law of nature, for now at least: compute per $ doubles every two years
        Chinchilla scaling laws: how much data versus compute is optimal for training Large Language Models (https://arxiv.org/abs/2203.15556)
        <br/><br/>
    </li>
        The pieces that are un-automatable tend to require what AI scientists call "grounding".
        In other words, real understanding. ChatGPT isn't grounded.<br/><br/>
        One example: ChatGPT can't accurately multiply large numbers.
        That's because it hasn't in its training set seen every literal combination of large numbers being multiplied. The amount of data it would need to see gets combinatorially explosive as the numbers get larger.
        The technical terminology for this is "intensivity versus extensivity", or "distributional shift".
        "distributional shift" is the phenomenon of the data model are trained on being (even somewhat) different from what it sees at inference time.
        <br/><br/>
        This isn't a John Henry thing...of course you can multiply numbers with a calculator.
        Or, you can write custom code that recognizes a given prompt as a multiplication problem and sends it to a "calculator" submodule.
        A calculator isn't AGI though.<br/><br/>
        The point is LLMs don't actually understand multiplication. Not yet anyway.
        <br/><br/>
    <li>
        This is similar to/a variant of Moravec's Paradox. What's easy for a human is hard for an AI. "It is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility" (https://lnkd.in/eetDKci6).
        <br/><br/>
    </li>
    <li>
        Fully-autonomous self-driving has been predicted for years, yet still isn't here. Ability to use AI to augment one’s work and increase productivity is real. However, that’s not replacement/singularity.
        <br/><br/>
    </li>
    <li>
        I've noticed a lot of opinions on the matter simply leave it there. Moravec's Paradox, real-world understanding, etc.: there is no singularity. I think there will be a singularity. The cortex has approximately the volume of a softball. It can't be that hard (understatement). In other words, AI will replace all jobs, but do so
        for all of them at about the same time. Step-change.
        <br/><br/>
    </li>
    <li>
        Coming at it from the other side, neural net architectures are converging. Transformers are effective in broader and broader problem domains. Karpathy has a good thread about this: tinyurl.com/26vc6b36. Brains use a single architecture for all these different problem domains, why wouldn't AI? There's even evidence (Nature paper!) of brains doing something roughly similar to next-token prediction: tinyurl.com/y88vne6n.
    </li>
    <br/><br/>
    I think it’ll be a species-level cross-profession transition, with a fairly concise timeline
</ul>
