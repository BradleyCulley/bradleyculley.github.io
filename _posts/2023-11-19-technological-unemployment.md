## <div align="center">Technological Unemployment<div>

<div align="center">
  <img src="https://bradleyculley.github.io/images/technological_unemployment_illustration1.png" />
</div>
<div align="center">
  <img src="https://bradleyculley.github.io/images/technological_unemployment_illustration2.png" />
</div>

**I think AI will automate all, or almost all, jobs at about the same time. In other words, a step-change profile for the technological singularity.**<br/><br/>

Reasoning:
* The key idea of this essay is: <b>I think the different jobs people do are not very computationally distinct.</b><br/><br/>
    Being a lawyer versus being a construction worker versus being an engineer.<br/><br/>
    They seem quite distinct. That's because, within the space of all possible computations, we're "zoomed in" on the kinds of computations that a human can get paid to do.<br/><br/>
    Further evidence, in both humans and AIs, is what in AI is called "transfer learning".<br/><br/>
    "Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task."
    [https://machinelearningmastery.com/transfer-learning-for-deep-learning](https://machinelearningmastery.com/transfer-learning-for-deep-learning)<br/><br/>
    Humans do transfer learning! It's easier for an engineer to become a doctor, than it is for a random person to become a doctor.<br/><br/>
    A background in engineering helps with becoming a lawyer, even.
    My father is a lawyer. He said people with STEM degrees did very well in law school.<br/><br/>
    Professor Yann LeCun makes the distinction between artificial general intelligence (AGI) and "human-level intelligence".
    LeCun is the head of AI at Facebook/Meta, and a Turing Award laureate (equivalent in computing of the Nobel Prize) for his foundational work on AI.<br/><br/>
    _LeCun says human intelligence is actually quite narrow._
    In the universe of every possible computation (the Ruliad), human intelligence is optimized for a few specific things.
    That's illustrated in the header images above.<br/><br/>

* As far as the jobs humans have, those jobs have already been optimized around the parts of the computational universe (the Ruliad) that machines can't handle.
        <br/><br/>
        Paradoxically, as we've automated more, to date there's been more that's needed doing. 
        We've always thought innovations, such as cars, would on balance destroy jobs. So far, they never have.<br/><br/>
        Two economic theories that explain this paradox:<br/>
          1. Joseph Schumpeter's "Creative destruction" and<br/>
          2. David Frederick Schloss' "Lump of Labor Fallacy"<br/><br/>
              Joseph Schumpeter's "Creative destruction" is about "the process that sees new innovations replacing existing ones that are rendered obsolete over time."<br/>
              -[https://www.cmu.edu/epp/irle/irle-blog-pages/schumpeters-theory-of-creative-destruction.html](https://www.cmu.edu/epp/irle/irle-blog-pages/schumpeters-theory-of-creative-destruction.html)
              <br/><br/>
              "As an example, in the late 1800s and early 1900s incremental improvements to horse and buggy transportation continued to be valuable, and innovations in the buggy and buggy whip could fetch a considerable price in the market.  With the introduction of Ford’s Model T in 1908, however, these “technologies” were effectively driven out by a superior innovation.  Over time, newer and better innovations will continue to drive out worse ones, just as the Model T did the horse and buggy and numerous iterations of vehicles have subsequently driven out the Model T and generations of its successors.
              -[https://www.cmu.edu/epp/irle/irle-blog-pages/schumpeters-theory-of-creative-destruction.html](https://www.cmu.edu/epp/irle/irle-blog-pages/schumpeters-theory-of-creative-destruction.html)
              <br/><br/>
              The point is that after switching from horses to cars, there were plenty of jobs building cars, fixing cars, and so on.
              In fact, more jobs in total than in the era of horses.
              <br/><br/>
              Closely related is David Frederick Schloss' "Lump of Labor Fallacy".
              <br/><br/>
              "The lump of labor fallacy is the assumption that there is a fixed amount of work to be done. If this were true, new jobs could not be generated, just redistributed. Those who believe the fallacy have often felt threatened by new technology or the entrance of new people into the labor force. These fears are rooted in a mistaken zero-sum view of the economy, which holds that when someone gains in a transaction, someone else loses. It's a tempting idea to some because it seems to be true. For example, jobs can be lost to automation and immigration. However, that is not the full story. In reality, the demand for labor is not fixed. Changes in one industry can be offset, or overshadowed, by growth in another. And as the labor force grows, total employment increases too (Figure 1)."<br/>
              -[https://research.stlouisfed.org/publications/page1-econ/2020/11/02/examining-the-lump-of-labor-fallacy-using-a-simple-economic-model](https://research.stlouisfed.org/publications/page1-econ/2020/11/02/examining-the-lump-of-labor-fallacy-using-a-simple-economic-model)
              <br/><br/>
              The total number of jobs, _and_ the total number of employed people, has basically just gone up for a long time:<br/>
              <img style="width: 50vw;" src="https://bradleyculley.github.io/images/labor_force_and_employment.png" />
              <br/><br/>
              In fact, there are more job openings than unemployed people, part of a 10-year trend:<br/>
              <img style="width: 50vw;" src="https://bradleyculley.github.io/images/unemployed_people_per_job_opening.png" /><br/><br/>
              The red line going below "1" in the graph means there are more jobs than unemployed people.
              It's solidly below 1 right now.
              <br/><br/>
              Creative Destruction and The Lump of Labor Fallacy aren't laws of nature though. They could stop being true any time.<br/><br/>
              "Connectionism", however, is actual science.
              Connectionism is the theory that thinking, memory, and so on are all enabled by the patterns of connections between neurons in the brain.<br/><br/>
              Ok, but what does that have to do with jobs? Well, ultimately human labor boils down to patterns of connections between neurons, and the firing of those neurons.
              No matter how fancy or well-paid the job, it's fundamentally just specific patterns of connected neurons firing.<br/><br/>
              Moore's Law may as well be a law of nature, for now at least. 
              Moore's Law states that compute per $ doubles every two years.<br/><br/>
              Likewise for the Chinchilla scaling laws, which guide how much data versus compute is optimal for training Large Language Models [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)
              <br/><br/>

*  The pieces that are un-automatable tend to require what AI scientists call "grounding".
        In other words, real understanding. ChatGPT isn't grounded.<br/><br/>
        One example: ChatGPT can't accurately multiply large numbers.
        That's because it hasn't in its training set seen every literal combination of large numbers being multiplied. The amount of data it would need to see gets combinatorially explosive as the numbers get larger.
        The technical terminology for this is "intensivity versus extensivity", or "distributional shift".
        "distributional shift" is the phenomenon of the data the model is trained on being different from what it sees at inference time.<br/><br/>
        This isn't a John Henry thing...of course you can multiply numbers with a calculator.
        Or, you can write custom code that recognizes a given prompt as a multiplication problem and sends it to a "calculator" submodule.
        A calculator isn't AGI though.<br/><br/>
        The point is LLMs don't actually understand multiplication. Not yet anyway.
        <br/><br/>

*   This is similar to/a variant of Moravec's Paradox. What's easy for a human is hard for an AI. "It is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility" (https://lnkd.in/eetDKci6).<br/><br/>

*   Fully-autonomous self-driving has been predicted for years, yet still isn't here.
    Ability to use AI to augment one’s work and increase productivity is real. But that’s not replacement or the singularity.<br/><br/>

*   "AI won't replace me - I'm a plumber!"<br/><br/>
        Robotics is a software problem. I don't think skilled trades like plumbing, carpentry, or transportation, are uniquely safe just because they don't take place on a computer.<br/><br/>
        Check out this robot from Boston Dynamics:<br/><br/>
        [https://www.youtube.com/embed/-e1_QhJ1EhQ?si=DmCFedhG7S6K7dPp](https://www.youtube.com/embed/-e1_QhJ1EhQ?si=DmCFedhG7S6K7dPp)<br/><br/>
        I think, with the right software, that robot, or one like it, could install shingles on a roof.
        Here's a video of a robotic hand system performing fine motor tasks via a human operator: [https://www.youtube.com/watch?v=xyqJ6_cdenI](https://www.youtube.com/watch?v=xyqJ6_cdenI)<br/><br/>
        Sensors and actuators aren't the limiting factor. Software is the limiting factor.<br/><br/>
        As it goes for computer programming, so it goes for plumbing, carpentry, and HVAC.
        <br/><br/>

*   "ChatGPT will replace all jobs".<br/><br/>
    "ChatGPT will replace job X".<br/>
    See above about "grounding".<br/>
    Large language models like ChatGPT arguably have exponentially diminishing accuracy as Kolmogorov complexity increases.
    This is a post from Yann LeCun about that: [https://twitter.com/ylecun/status/1640122342570336267?lang=en](https://twitter.com/ylecun/status/1640122342570336267?lang=en).
    This illustration from LeCun's post is a good summary of his argument:
    <div align="center" style="width: 50vw;">
        <img src="https://bradleyculley.github.io/images/exponential_divergence_in_LLMs.jpeg" />
    </div><br/><br/>

*   I've noticed a lot of opinions on the matter simply leave it there.
    Moravec's Paradox, real-world understanding, etc.: there is no singularity.
    I think there will be a singularity. The cortex has approximately the volume of a softball.
    It can't be that hard to create human-level AI (understatement).
    In other words, AI will replace all jobs, but it will do so for all of them at about the same time. Step-change.<br/><br/>

*   Coming at it from the other side, neural net architectures are converging.
    Transformers/LLMs are effective in broader and broader problem domains.
    Karpathy has a good thread about this: [tinyurl.com/26vc6b36](tinyurl.com/26vc6b36).
    Google is using transformers in its robotics: [https://blog.google/technology/ai/google-deepmind-rt2-robotics-vla-model/](https://blog.google/technology/ai/google-deepmind-rt2-robotics-vla-model/).
    Brains use a single architecture for all these different problem domains, why wouldn't AI?
    There's even evidence (Nature paper!) of brains doing something roughly similar to next-token prediction: [https://www.nature.com/articles/s41562-022-01516-2](https://www.nature.com/articles/s41562-022-01516-2).

*   I think it’ll be a species-level cross-profession transition, with a fairly concise timeline.
The question then becomes: if it's everyone at once, how will we handle this?
